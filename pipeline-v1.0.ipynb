{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ce2178-ac16-496b-b2bc-6ebbe122bad8",
   "metadata": {},
   "source": [
    "# Transcription Pipeline for Speech-Language Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1d411c-afed-407c-8f9e-ef4735e9f83a",
   "metadata": {},
   "source": [
    "## Before you run the notebook..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286564b-40bd-4c9a-9dcf-cb4abf98c0ec",
   "metadata": {},
   "source": [
    "1. All libraries below should be installed.\n",
    "2. Have your HuggingFace token ready, along with other personal inputs for the following section.\n",
    "3. All `.wav` files to transcript should be prepared in your `raw_audio_folder`.\n",
    "4. Be sure user settings are correct in the first code block below.\n",
    "\n",
    "Otherwise, **you should be able to run the entire notebook to process your audio files**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ff50e-6426-4357-8157-1ccc71d1e280",
   "metadata": {},
   "source": [
    "## Customize User Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517ff95-cf86-49ac-912c-41df0cd5d963",
   "metadata": {},
   "source": [
    "This next section includes all personalized sections. Change all elements to reflect your device, data, and settings. Everything afterward should run without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e09a2-4f14-4a22-9ae9-0d80d3138453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. User Settings ---\n",
    "WORKING_DIR = r\"C:\\Users\\USERNAME\\Documents\\asr\" #your directory goes here\n",
    "AUDIO_FOLDER = \"raw_audio_folder/\"\n",
    "OUTPUT_FOLDER = \"transcription_output/\"\n",
    "\n",
    "# --- 2. HuggingFace Token ---\n",
    "HUGGINGFACE_TOKEN = \"YOUR_TOKEN_HERE",
    "\n",
    "# --- 3. Whisper Model Choice ---\n",
    "WHISPER_MODEL = \"base.en\"  # Options: \"tiny.en\", \"base.en\", \"small.en\", \"medium.en\", \"large-v3\", etc.\n",
    "\n",
    "# --- 4. Processing Options ---\n",
    "DIARIZATION = True   # True to enable speaker diarization, otherwise False\n",
    "LEVEL = \"WORD\"    # Options: \"WORD\" or \"SEGMENT\"\n",
    "EXPORT_AS = \"CSV\"    # Options: \"CSV\" or \"TXT\"\n",
    "\n",
    "print(\"User settings loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426ea4b-1928-4c8a-8536-94cb9b2ea715",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902eaa34-ec67-4175-9673-3f6d9800ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Required Libraries ---\n",
    "import stable_whisper\n",
    "import pandas as pd # aliases are often used to avoid having to refer to the library by its full name \n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73601c40-bfa1-422a-a51f-4289cfae8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "from pyannote.core import Timeline, Segment\n",
    "from pyannote.database.util import load_rttm\n",
    "from pyannote.audio import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750852f-fe87-4df9-a431-6c4e70793822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac9401-aa4a-453d-849c-dc1d5e49c497",
   "metadata": {},
   "source": [
    "## Load Whisper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554efdcb-8176-4bce-9c4d-7f90d2851510",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = stable_whisper.load_model(WHISPER_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa73bc3-a7bb-405c-8904-9777386db51a",
   "metadata": {},
   "source": [
    "## Set Up Directories and Diarization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0765eb-858f-49f3-8d60-adf8cf79d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set Working Directory ---\n",
    "os.chdir(WORKING_DIR)\n",
    "os.makedirs(AUDIO_FOLDER, exist_ok=True)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# --- Set up Pyannote Pipeline if Diarization is Enabled ---\n",
    "if (DIARIZATION == True):\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=HUGGINGFACE_TOKEN\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    pipeline.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b5870-1012-4597-b76e-2396dca2a14f",
   "metadata": {},
   "source": [
    "## RUN WHISPER AND PYANNOTE ON ALL .WAV FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed881fc-ceda-4daa-b7b4-cd98bd107e68",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b4bd3-0d33-476e-88b2-afcebec2235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function to Check Overlap ---\n",
    "def is_overlap(start1, end1, start2, end2):\n",
    "    return max(start1, start2) < min(end1, end2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c3fb6-25e3-431b-ad9d-a36911c9896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize(audio_file_path):\n",
    "    \n",
    "    with ProgressHook() as hook:\n",
    "          diarization = pipeline(audio_file_path, hook=hook, min_speakers=2, max_speakers=2) #<--this indicates anticipated # of speakers\n",
    "    \n",
    "    diarization_list = [{\n",
    "                'start': segment.start,\n",
    "                'end': segment.end,\n",
    "                'speaker': list(track.values())[0]\n",
    "            } for segment, track in diarization._tracks.items()]\n",
    "\n",
    "    diarization_df = pd.DataFrame(diarization_list)\n",
    "\n",
    "    return diarization_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad84d4-4bc8-4220-a26b-231dab591dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_diarization_and_transcription(speaker_segs_df, df_segments):\n",
    "    \n",
    "    labels = []\n",
    "    predominant_labels = []\n",
    "    max_overlaps = []\n",
    "    durations = []\n",
    "    all_overlaps = []\n",
    "    \n",
    "      \n",
    "    for i, row in df_segments.iterrows():\n",
    "        \n",
    "        overlaps = speaker_segs_df.apply(\n",
    "            lambda x: (x['speaker'], min(row['end'], x['end']) - max(row['start'], x['start'])) \n",
    "            if is_overlap(row['start'], row['end'], x['start'], x['end']) else (None, 0), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Filter out non-overlapping entries\n",
    "        overlaps = overlaps[overlaps.apply(lambda x: x[0] is not None)]\n",
    "        \n",
    "        # Extract labels and their corresponding overlap times\n",
    "        overlapping_labels = overlaps.apply(lambda x: x[0])\n",
    "        \n",
    "        overlapping_labels.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        labels.append(\" \".join(overlapping_labels))\n",
    "        \n",
    "        # Initialize a defaultdict to store the summed values\n",
    "        collapsed_dict = defaultdict(float)\n",
    "        \n",
    "        # Iterate over the series and sum values for each key\n",
    "        for item in overlaps:\n",
    "            key, value = item\n",
    "            collapsed_dict[key] += value\n",
    "        \n",
    "        # Convert the defaultdict back to a list of tuples \n",
    "        collapsed_list = list(collapsed_dict.items())\n",
    "        non_empty_label_overlap = [item for item in collapsed_list if item[0] is not None]\n",
    "        collapsed_series = pd.Series(collapsed_list)\n",
    "        \n",
    "        overlap_times = collapsed_series.apply(lambda x: x[1])\n",
    "        overlap_labels = collapsed_series.apply(lambda x: x[0])\n",
    "        \n",
    "        \n",
    "        overlap_times.reset_index(drop=True, inplace=True)\n",
    "        overlap_labels.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Determine the predominant label (the one with the greatest overlap time)\n",
    "        if not overlap_times.empty:\n",
    "           # print(overlap_times)\n",
    "            predominant_label = overlap_labels.iloc[overlap_times.idxmax()]\n",
    "            predominant_labels.append(predominant_label)\n",
    "            max_overlap = overlap_times.iloc[overlap_times.idxmax()]\n",
    "            max_overlaps.append(max_overlap)\n",
    "            all_overlaps.append(non_empty_label_overlap)\n",
    "        else:\n",
    "            predominant_labels.append(\"\")\n",
    "            max_overlaps.append(\"\")\n",
    "            all_overlaps.append(\"\")\n",
    "        \n",
    "    df_segments[\"predominant_speaker\"] = predominant_labels\n",
    "\n",
    "    return df_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ad001-cf66-41f6-bd2b-7b9855f532f6",
   "metadata": {},
   "source": [
    "### Main Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26899d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_file(model, audio_file_path, level, diarization = False):\n",
    "    result = model.transcribe(audio_file_path)\n",
    "\n",
    "    data = result.ori_dict\n",
    "\n",
    "    if diarization == True:\n",
    "        diarization_df = diarize(audio_file_path)\n",
    "\n",
    "\n",
    "    if level == \"WORD\":\n",
    "        words_data = []\n",
    "        for segment in data['segments']:\n",
    "            for word in segment['words']:\n",
    "                words_data.append({\n",
    "                    'word': word['word'],\n",
    "                    'start': word['start'],\n",
    "                    'end': word['end'],\n",
    "                    'probability': word['probability']\n",
    "                })\n",
    "        \n",
    "        timestamp_df = pd.DataFrame(words_data)\n",
    "\n",
    "    elif level == \"SEGMENT\":\n",
    "        utterances_data = []\n",
    "        for segment in data['segments']:\n",
    "            utterances_data.append({\n",
    "                'text': segment['text'],\n",
    "                'start': segment['start'],\n",
    "                'end': segment['end'],\n",
    "                'id': segment['id']\n",
    "            })\n",
    "        \n",
    "        timestamp_df = pd.DataFrame(utterances_data)\n",
    "\n",
    "    if diarization == True:\n",
    "        diarization_and_timestamp_df = align_diarization_and_transcription(diarization_df, timestamp_df)\n",
    "        return diarization_and_timestamp_df \n",
    "    else:\n",
    "        return timestamp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35412c9-bbb9-422a-ab9d-692747bc65d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_textgrid(df):\n",
    "    # Create a new TextGrid object\n",
    "    tg = textgrid.Textgrid()\n",
    "    \n",
    "    min_time = df['start'].min()\n",
    "    max_time = df['end'].max()\n",
    "    \n",
    "    # Check if 'predominant_speaker' column exists\n",
    "    has_speaker_info = 'predominant_speaker' in df.columns\n",
    "    \n",
    "    if has_speaker_info:\n",
    "        # Create an IntervalTier for each unique speaker\n",
    "        speakers = df['predominant_speaker'].unique()\n",
    "        for speaker in speakers:\n",
    "            # Create a tier for the speaker\n",
    "            speaker_tier =  IntervalTier(str(speaker), [], minT=min_time, maxT=max_time)\n",
    "            \n",
    "            # Filter the DataFrame for this speaker\n",
    "            speaker_df = df[df['predominant_speaker'] == speaker]\n",
    "            \n",
    "            # Add intervals to the tier\n",
    "            for _, row in speaker_df.iterrows():\n",
    "                interval = Interval(row['start'], row['end'], row['word'])\n",
    "                speaker_tier.insertEntry(interval, collisionMode='merge')\n",
    "            \n",
    "            # Add tier to TextGrid\n",
    "            tg.addTier(speaker_tier)\n",
    "    \n",
    "    # Create a transcription tier (this will be created regardless of speaker info)\n",
    "    trans_tier = IntervalTier('transcription', [], minT=min_time, maxT=max_time)\n",
    "\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        interval = Interval(row['start'], row['end'], row['text'])\n",
    "        trans_tier.insertEntry(interval, collisionMode='merge')\n",
    "    \n",
    "    tg.addTier(trans_tier)\n",
    "    \n",
    "    return tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee26b1-578d-4e99-9168-b4a121dae45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_folder = AUDIO_FOLDER\n",
    "output_folder = OUTPUT_FOLDER\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(audio_folder):\n",
    "    if filename.lower().endswith(('.wav', '.mp3', '.flac')):  # Add or remove audio formats as needed\n",
    "        audio_file_path = os.path.join(audio_folder, filename)\n",
    "        \n",
    "        print(f\"Processing: {filename}\")\n",
    "        \n",
    "        # Process the audio file\n",
    "        df_transcript = process_audio_file(model, audio_file_path, level = LEVEL, diarization = DIARIZATION)\n",
    "        \n",
    "        # Generate output filename (without extension)\n",
    "        output_filename = os.path.splitext(filename)[0]\n",
    "        \n",
    "        if EXPORT_AS == \"CSV\":\n",
    "            output_path = os.path.join(output_folder, f\"{output_filename}.csv\")\n",
    "            df_transcript.to_csv(output_path, index=False)\n",
    "        elif EXPORT_AS == \"TXT\":\n",
    "            output_path = os.path.join(output_folder, f\"{output_filename}.txt\")\n",
    "            with open(output_path, 'w') as f:\n",
    "                df_string = df_transcript.to_string(header=False, index=False)\n",
    "                f.write(df_string)\n",
    "        else:\n",
    "            print(f\"Unsupported export format: {export_format}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Saved transcription to: {os.path.basename(output_path)}\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
